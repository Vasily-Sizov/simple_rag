LLM RAG Приложение
=================

Веб-приложение на Django для работы с PDF документами с использованием Saiga LLM модели и RAG подхода.

Требования
---------
- Python 3.10+
- Poetry
- 8+ ГБ RAM
- CUDA-совместимая видеокарта с 16+ ГБ VRAM

Установка
--------
1. Клонируйте репозиторий:
   git clone https://github.com/yourusername/llm-rag-app.git
   cd llm-rag-app

2. Установите зависимости (два варианта):

   С помощью Poetry (рекомендуется):
   poetry install
   poetry shell

   Или с помощью pip:
   python -m venv venv
   source venv/bin/activate  # Для Linux/MacOS
   # или
   venv\Scripts\activate  # Для Windows
   pip install -r requirements.txt

3. Скачайте и настройте LLM модель:
   - Создайте директорию для модели в проекте:
     mkdir -p rag_service/models

   Модель будет автоматически скачана при первом запуске приложения
   и сохранена в директории rag_service/models/saiga_llama3

4. Перейдите в директорию проекта и выполните миграции:
   cd rag_service
   python manage.py migrate

5. Запустите сервер разработки:
   python manage.py runserver

Использование
-----------
1. Откройте браузер и перейдите по адресу http://localhost:8000
2. Загрузите PDF файл через интерфейс
3. После успешной загрузки и обработки файла, вы можете задавать вопросы по его содержимому
4. Система использует RAG (Retrieval-Augmented Generation) для поиска релевантной информации в документе и генерации ответов

Архитектура
----------
Приложение использует следующие компоненты:
- Django для веб-интерфейса
- Saiga Llama3 8B для генерации ответов
- FAISS (Facebook AI Similarity Search) для эффективного векторного поиска
- LangChain Framework для обработки документов и организации RAG пайплайна
- rubert-tiny-turbo для создания эмбеддингов (специально оптимизирован для русского языка)

Векторный поиск с FAISS
-------------------
1. Создание эмбеддингов:
   - Текст разбивается на чанки (300 символов с перекрытием 100)
   - Каждый чанк преобразуется в вектор с помощью rubert-tiny-turbo
   - Размерность векторов: 312 (определяется моделью эмбеддингов)

2. Индексация в FAISS:
   - Используется индекс IndexFlatL2 (точный поиск по L2-расстоянию)
   - Векторы нормализуются для улучшения качества поиска
   - Индекс хранится в оперативной памяти для быстрого доступа

3. Поиск релевантных чанков:
   - Вопрос пользователя преобразуется в вектор
   - FAISS находит k ближайших соседей (k=2)
   - Используется косинусное расстояние для определения схожести
   - Результаты фильтруются по максимальной длине (600 символов)

4. Оптимизации:
   - Асинхронная загрузка индекса
   - Кэширование часто используемых векторов
   - Автоматическая очистка памяти при необходимости

5. Интеграция с LangChain:
   - FAISS интегрирован через langchain_community.vectorstores
   - Автоматическая обработка метаданных документов
   - Поддержка инкрементальных обновлений индекса

Использование LangChain
-------------------
1. Обработка документов:
   - RecursiveCharacterTextSplitter для разбиения текста на чанки
      * Размер чанка: 300 символов
      * Перекрытие: 100 символов
      * Умное разбиение по разделителям ["\n\n", "\n", ".", " ", ""]
      * Сохранение смысловой целостности текста

2. Работа с эмбеддингами:
   - HuggingFaceEmbeddings для создания векторных представлений
      * Модель: rubert-tiny-turbo
      * Автоматическая батчевая обработка
      * Кэширование эмбеддингов

3. Векторное хранилище:
   - FAISS интеграция через langchain_community.vectorstores
      * Автоматическое создание и обновление индекса
      * Управление метаданными документов
      * Эффективный поиск похожих документов

4. RAG пайплайн:
   - Загрузка PDF через pypdf
   - Извлечение и очистка текста
   - Разбиение на чанки с помощью TextSplitter
   - Создание эмбеддингов и индексация
   - Семантический поиск при получении вопроса

5. Преимущества использования LangChain:
   - Модульная архитектура
   - Готовые компоненты для работы с документами
   - Простая интеграция с различными моделями и базами данных
   - Оптимизированные алгоритмы обработки текста
   - Встроенные механизмы кэширования

Поддерживаемые LLM модели
----------------------
1. Saiga Llama3 8B (основная модель)
   - Разработчик: AI Forever
   - Размер: 8 миллиардов параметров
   - Базовая архитектура: Llama 3
   - Особенности:
     * Оптимизирована для русского языка
     * Основана на Llama 3 с дополнительным обучением
     * Поддерживает диалоговый формат
     * Отличные результаты в бенчмарках
   - Требования к памяти: ~8GB VRAM
   - Рекомендуется для: работы с русскоязычными текстами

Особенности реализации
-------------------
1. Обработка документов:
   - Размер чанка: 300 символов
   - Перекрытие чанков: 100 символов
   - Фильтрация чанков: 15-100 слов
   - Максимальная длина контекста: 600 символов

2. Промпт-инжиниринг:
   - Строгие инструкции для модели
   - Нумерация чанков контекста
   - Требование цитирования
   - Примеры правильных и неправильных ответов

3. Параметры генерации:
   - temperature: 0.1 (строгое следование контексту)
   - do_sample: False (детерминированная генерация)
   - max_new_tokens: 256
   - repetition_penalty: 1.2

Оптимизации моделей
-----------------
Все модели используют следующие оптимизации для эффективной работы:
- Float16 precision для уменьшения использования памяти
- Автоматическое распределение слоев модели по доступным устройствам
- Оптимизированное использование CPU и GPU памяти
- Компиляция моделей с помощью torch.compile
- Периодическая очистка GPU памяти
- Прогрев модели при инициализации

Интерфейс
---------
Приложение имеет три основных страницы:
1. Чат:
   - Загрузка PDF файлов
   - Диалоговый интерфейс
   - Отображение контекста и инструкций

2. Документ:
   - Просмотр всех чанков документа
   - Анализ разбиения текста

3. Мониторинг:
   - Grafana дашборд
   - Метрики использования GPU/CPU
   - Статистика запросов

Требования к оборудованию
----------------------
- Минимальные: NVIDIA GPU с 16GB VRAM (например, RTX 4080)
- Рекомендуемые: NVIDIA GPU с 24GB VRAM (например, RTX 4090 или RTX 3090)
- CPU: не рекомендуется из-за размера модели

Процесс работы:
1. Обработка документа через LangChain:
   - Загрузка PDF с помощью pypdf
   - Извлечение текста и базовая очистка
   - Разбиение на чанки с помощью RecursiveCharacterTextSplitter
   - Фильтрация чанков по длине (15-100 слов)

2. Векторизация и индексация:
   - Каждый чанк преобразуется в вектор с помощью rubert-tiny-turbo
   - Векторы индексируются в FAISS для быстрого поиска
   - LangChain создает и поддерживает связи между векторами и текстом
   - Метаданные документов сохраняются для дополнительного контекста

3. При получении вопроса:
   - Вопрос преобразуется в вектор тем же эмбеддингом
   - FAISS находит семантически близкие чанки по векторам
   - Отбираются наиболее релевантные чанки в пределах лимита
   - LangChain формирует структурированный промпт с контекстом
   - Генерирует ответ с помощью Saiga Llama3
   - Возвращает структурированный ответ с контекстом и инструкциями

Структура проекта
---------------
llm_rag_app/
├── README.txt
├── pyproject.toml
├── .gitignore
├── requirements.txt
├── rag_service/
│   ├── manage.py
│   ├── rag_service/
│   │   ├── __init__.py
│   │   ├── settings.py
│   │   ├── urls.py
│   │   └── wsgi.py
│   ├── chat/
│   │   ├── __init__.py
│   │   ├── apps.py
│   │   ├── models.py
│   │   ├── services/
│   │   │   ├── __init__.py
│   │   │   ├── document_processor.py
│   │   │   ├── llm_service.py
│   │   │   └── vector_store.py
│   │   ├── templates/
│   │   │   └── chat/
│   │   │       ├── index.html
│   │   │       └── chat.html
│   │   ├── urls.py
│   └── static/
│       ├── css/
│       │   └── main.css
│       └── js/
│           └── chat.js

Настройка
--------
По молчанию приложение использует CPU для работы с моделью. Для использования GPU:
1. Установите CUDA toolkit
2. В файле rag_service/chat/services/llm_service.py добавьте параметр device='cuda' при инициализации модели

Расположение модели:
По молчанию модель будет автоматически загружена из Hugging Face
(модель "IlyaGusev/saiga_llama3_8b")

Расположение кэша моделей:
- Windows: C:\Users\<username>\.cache\huggingface\hub
- Linux/MacOS: ~/.cache/huggingface/hub

В кэше хранятся:
- Saiga Llama3 модель
- rubert-tiny-turbo для эмбеддингов
- Конфигурационные файлы моделей

Для изменения расположения кэша можно установить переменную окружения:
TRANSFORMERS_CACHE=/путь/к/новому/кэшу

Если вы хотите использовать модель из другого расположения, измените путь в файле
rag_service/chat/services/saiga_service.py:
    self.model_name = "путь/к/вашей/модели"

Ограничения
----------
- Приложение оптимизировано для работы с текстовыми PDF файлами
- Размер обрабатываемого PDF файла ограничен доступной памятью
- Качество ответов зависит от качества входного документа и релевантности найденых чанков

Разработка
---------
Для разработки используются:
- black для форматирования кода
- flake8 для проверки стиля
- isort для сортировки импортов

Запуск линтеров:
black .
flake8 .
isort .

Лицензия
--------
MIT

Поддержка
--------
При возникновении проблем создавайте issue в репозитории проекта. 
